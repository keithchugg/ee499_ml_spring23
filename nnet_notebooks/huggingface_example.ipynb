{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Pipeline Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Huggingface is the most popular open-source library for natural language processing (NLP) models and tasks\n",
    "- Pipeline is a high-level API tool provided by Huggingface that makes it easy to perform various NLP tasks using pre-trained models with minimal coding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch and tensorflow are the computational backend for training and running deep learning models like BERT, which is the backbone of the Transformers library. Therefore we need to install one of them to be able to use the transformers library. Make sure you have Pytorch installed and install the  `transformers` using `pip install transformers`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Using BERT for masked-language modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the tasks used for pre-training BERT is masked language modeling (MLM). In MLM, a certain percentage of the tokens in a sentence are randomly replaced with a special token, such as [MASK], and the goal is to predict the original token(s) that were replaced. \n",
    "\n",
    "Since BERT was designed for this task, we can directly use the original pre-trained weights of BERT to perform MLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "from transformers import pipeline\n",
    "model = pipeline(\"fill-mask\", model=\"bert-base-uncased\") # This will download and cache the model if it is not already downloaded (bert is approx 500MB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the capital city of paris is known for the eiffel tower.               with score: 0.3470%\n",
      "the capital city of luxembourg is known for the eiffel tower.          with score: 0.2427%\n",
      "the capital city of france is known for the eiffel tower.              with score: 0.1014%\n",
      "the capital city of brussels is known for the eiffel tower.            with score: 0.0736%\n",
      "the capital city of monaco is known for the eiffel tower.              with score: 0.0530%\n"
     ]
    }
   ],
   "source": [
    "query_text = \"The capital city of [MASK] is known for the Eiffel Tower.\"\n",
    "\n",
    "results = model(query_text)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"{result['sequence']:<70} with score: {result['score']:.4f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Bert for Sentiment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a different task. Sentiment analysis is a popular task in NLP. The original BERT model was not trained on this task. We need some model fine-tuned on sentiment analysis.\n",
    "Let's use the distillbert model fine-tuned on the sst-2 dataset. Distilbert is a lighter version of BERT (half the size) and SST-2 is the \"Stanford Sentiment Treebank\", a dataset of 11,855 sentences of movie reviews where each sentence is labeled as either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = pipeline(task='sentiment-analysis',model=\"distilbert-base-uncased-finetuned-sst-2-english\") # This will download and cache the model if it is not already downloaded (distilbert is approx 250MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a fantastic movie!               predicted label: POSITIVE   with score: 0.9999%\n",
      "Meh, I didn't like it that much.         predicted label: NEGATIVE   with score: 0.9989%\n",
      "Liked the acting, not much the plot.     predicted label: POSITIVE   with score: 0.5962%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reviews = [\n",
    "    \"This is a fantastic movie!\",\n",
    "    \"Meh, I didn't like it that much.\",\n",
    "    \"Liked the acting, not much the plot.\",\n",
    "]\n",
    "\n",
    "results = model(reviews)\n",
    "\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"{reviews[i]:<40} predicted label: {result['label']:<10} with score: {result['score']:.4f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
