{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3 - PCA Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST_data(data_path, fashion=False, quiet=False):\n",
    "    if not fashion:\n",
    "        train_set = datasets.MNIST(data_path, download=True, train=True)\n",
    "        test_set = datasets.MNIST(data_path, download=True, train=False)\n",
    "    else:\n",
    "        train_set = datasets.FashionMNIST(data_path, download=True, train=True)\n",
    "        test_set = datasets.FashionMNIST(data_path, download=True, train=False)      \n",
    "    x_train = train_set.data.numpy()\n",
    "    y_train = train_set.targets.numpy()\n",
    "\n",
    "    x_test = test_set.data.numpy()\n",
    "    y_test = test_set.targets.numpy()\n",
    "    \n",
    "    N_train, H, W = x_train.shape\n",
    "    N_test, H, W = x_test.shape\n",
    "\n",
    "    if not quiet:\n",
    "        print(f'The data are {H} x {W} grayscale images.')\n",
    "        print(f'N_train = {N_train}')\n",
    "        print(f'N_test = {N_test}')\n",
    "    for i in set(y_train):\n",
    "        N_i_train = np.sum(y_train==i)\n",
    "        N_i_test = np.sum(y_test==i)\n",
    "        if not quiet:\n",
    "            print(f'Class {i}: has {N_i_train} train images ({100 * N_i_train / N_train : .2f} %), {N_i_test} test images ({100 * N_i_test/ N_test : .2f} %) ')\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing and Inspecting the Data\n",
    "First, let's get a function to get the MNIST or FMNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "The data are 28 x 28 grayscale images.\n",
      "N_train = 60000\n",
      "N_test = 10000\n",
      "Class 0: has 6000 train images ( 10.00 %), 1000 test images ( 10.00 %) \n",
      "Class 1: has 6000 train images ( 10.00 %), 1000 test images ( 10.00 %) \n",
      "Class 2: has 6000 train images ( 10.00 %), 1000 test images ( 10.00 %) \n",
      "Class 3: has 6000 train images ( 10.00 %), 1000 test images ( 10.00 %) \n",
      "Class 4: has 6000 train images ( 10.00 %), 1000 test images ( 10.00 %) \n",
      "Class 5: has 6000 train images ( 10.00 %), 1000 test images ( 10.00 %) \n",
      "Class 6: has 6000 train images ( 10.00 %), 1000 test images ( 10.00 %) \n",
      "Class 7: has 6000 train images ( 10.00 %), 1000 test images ( 10.00 %) \n",
      "Class 8: has 6000 train images ( 10.00 %), 1000 test images ( 10.00 %) \n",
      "Class 9: has 6000 train images ( 10.00 %), 1000 test images ( 10.00 %) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "USE_FASHION_MNIST = True\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_MNIST_data('./data/', fashion=USE_FASHION_MNIST, quiet=False)\n",
    "\n",
    "\n",
    "if USE_FASHION_MNIST:\n",
    "    tag_name = 'FashionMNIST'\n",
    "    label_names = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\", \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
    "else:\n",
    "    tag_name = 'MNIST'\n",
    "    label_names = [f'{i}' for i in set(y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gray() # B/W Images\n",
    "plt.figure(figsize = (10,9)) # Adjusting figure size\n",
    "# Displaying a grid of 3x3 images\n",
    "for i in range(9):\n",
    "    index = np.random.randint(low=0, high=len(y_train), dtype=int)\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_train[index])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Multiclass Classification\n",
    "Fist, define a couple helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_augment(x):\n",
    "    N, H, W = x.shape\n",
    "    D = H * W\n",
    "    x_aug = np.ones((N, D + 1))\n",
    "    x_aug[:, 1:] = x.reshape((N, D))\n",
    "    return x_aug\n",
    "\n",
    "def create_target_matrix(labels):\n",
    "    label_vals = list(set(labels))\n",
    "    C = len(label_vals)\n",
    "    N = len(labels)\n",
    "    y = -1 * np.ones((N, C)) \n",
    "    for n in range(N):\n",
    "        y[n][labels[n]] = +1\n",
    "    return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these to get augment the data (add the 1s) and then flatten.  Then, we create the target matrix ${\\bf Y}$ which is a $(N \\times C)$ matrix.  The $n^{th}$ row of this matrix has the correct class labels for the $n^{th}$ data point.  We adopt the convention that this is $-1$ at all columns (classes) except the correct class, for which it is $+1$.\n",
    "\n",
    "After solving the MSE regression with these ${\\bf Y}$ targets, we classify by maximizing over $g_m({\\bf x}) = {\\bf w}_m {\\bf x}$.  This can be viewed as doing a one-vs-rest (OvR) approach where each of the $C$ classifiers are designed as binary MSE classifiers, then using the fushion/decision rule of choosing the max.  Alternatively, it is a Maximal Value Method (MVM) where the weight vectors are optimized with the MSE criterion and the multiclass targets.  \n",
    "\n",
    "We will use the `np.linalg.lstsq()` routine to solve the MSE regression problem.  This is the pseudo-inverse appoach, not gradient descent!\n",
    "The `np.linalg.lstsq()` routine can handle passing the matrix targets ${\\bf Y}$ instead of vector targets ${\\bf y}_c$ -- i.e., the $c$-th column of ${\\bf Y}$.  One call with a matrix of targets is equivalent to $C$ calls with the associated vectors targets $\\{{\\bf y}_c\\}_{c=1}^C$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PCA dimension Reduction\n",
    "\n",
    "You need to perform PCA to get a reduced dimension of the training data. Following that, you would need to augment your training and test data, and find W_hat. You need to report the accuracy based on the obtained weights.\n",
    "\n",
    "A plot of the accuracy vs. dimension of the input is helpful. Hint: Accuracy increases as we increase the dimension of the input.\n",
    "\n",
    "The PCA procedure was demonstrated in lecture using NumPY methods, see [8.1_PCA_kMeans_MNIST.ipynb](https://github.com/keithchugg/ee499_ml_spring23/blob/main/8.1_PCA_kMeans_MNIST.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_aug = flatten_and_augment(x_train) #x_train need to be the training data with the reduced dimension\n",
    "x_test_aug = flatten_and_augment(x_test) #x_test need to be the test data with the reduced dimension\n",
    "Y = create_target_matrix(y_train)\n",
    "\n",
    "W_hat = np.linalg.lstsq(x_train_aug, Y, rcond=None)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mls23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f2b55be8af22e299bb14a990e5306b14c0e04e0c371124ebf94aa533852bd32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
