{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructors**: Prof. Keith Chugg (chugg@usc.edu) & Prof. Antonio Ortega (aortega@usc.edu)\n",
    "\n",
    "**Teaching Assistant**: Alexios Rustom (arustom@usc.edu)\n",
    "\n",
    "**Book**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). Machine learning refined: Foundations, algorithms, and applications. Cambridge University Press.\n",
    "\n",
    "**Notebooks**: Written by Alexios Rustom (arustom@usc.edu) and Prof. Keith Chugg (chugg@usc.edu). These notebooks are based on the following **Github repository**: [notebooks](https://github.com/jermwatt/machine_learning_refined/tree/gh-pages/presentations/2nd_edition/jupyter%20%2B%20reveal.js%20version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Order Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from matplotlib import cm\n",
    "import copy\n",
    "# import automatic differentiator to compute gradient module\n",
    "from autograd import grad\n",
    "from PIL import Image\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(w):\n",
    "\treturn w ** 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "r_min, r_max = -4.0, 4.0\n",
    "inputs = np.arange(r_min, r_max, 0.1)\n",
    "results = objective(inputs)\n",
    "ax1.plot(inputs, results, 'k')\n",
    "ax1.set_xlabel('w', fontsize=12)\n",
    "ax1.set_ylabel('g(w)', fontsize=12)\n",
    "ax2.plot(inputs, results, 'k')\n",
    "ax2.set_xlabel('w', fontsize=12)\n",
    "ax2.set_ylabel('g(w)', fontsize=12)\n",
    "ax2.plot(inputs[np.argmin(results)], min(results),'gD' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(w):\n",
    "\treturn -1 * w ** 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "r_min, r_max = -4.0, 4.0\n",
    "inputs = np.arange(r_min, r_max, 0.1)\n",
    "results = objective(inputs)\n",
    "ax1.plot(inputs, results, 'k')\n",
    "ax1.set_xlabel('w', fontsize=12)\n",
    "ax1.set_ylabel('g(w)', fontsize=12)\n",
    "ax2.plot(inputs, results, 'k')\n",
    "ax2.set_xlabel('w', fontsize=12)\n",
    "ax2.set_ylabel('g(w)', fontsize=12)\n",
    "ax2.plot(inputs[np.argmax(results)], max(results),'gD' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(w):\n",
    "\treturn np.sin(3 * w) + 0.1 * w ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "r_min, r_max = -4.0, 4.0\n",
    "inputs = np.arange(r_min, r_max, 0.001)\n",
    "results = objective(inputs)\n",
    "peaks_max, _ = find_peaks(results)\n",
    "peaks_min, _ = find_peaks(-results)\n",
    "ax1.plot(inputs, results, 'k')\n",
    "ax1.set_xlabel('w', fontsize=12)\n",
    "ax1.set_ylabel('g(w)', fontsize=12)\n",
    "ax2.plot(inputs, results, 'k')\n",
    "ax2.set_xlabel('w', fontsize=12)\n",
    "ax2.set_ylabel('g(w)', fontsize=12)\n",
    "ax2.plot(inputs[peaks_max], results[peaks_max], \"gD\")\n",
    "ax2.plot(inputs[peaks_min], results[peaks_min], \"gD\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(w):\n",
    "\treturn w ** 2 + 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling_minimum(inputs, n_samples, objective_value, case):\n",
    "    accepted_samples = []\n",
    "    accepted_samples_objective = []\n",
    "    minimum = 1e30\n",
    "    k = 0\n",
    "    while k < n_samples:\n",
    "        sample = np.random.choice(inputs)\n",
    "        if objective(sample) < minimum:\n",
    "            accepted_samples.append(sample) \n",
    "            minimum = objective(sample)\n",
    "            accepted_samples_objective.append(minimum)\n",
    "            if abs(minimum-objective_value) <= 1e-3:\n",
    "                print(f\"Case {case} reached convergence with {k+1} samples\")\n",
    "                break\n",
    "            k = k + 1\n",
    "    return accepted_samples, accepted_samples_objective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_min, r_max = -4.0, 4.0\n",
    "inputs = np.arange(r_min, r_max, 0.001)\n",
    "number_samples = [4, 40]\n",
    "plt.figure(figsize=(10,3))\n",
    "objective_value  = 0.2\n",
    "for n, n_samples in enumerate(number_samples):\n",
    "    ax = plt.subplot(1, 2, n + 1)\n",
    "    results = objective(inputs)\n",
    "    ax.plot(inputs, results,'k')\n",
    "    ax.set_xlabel('w', fontsize=12)\n",
    "    ax.set_ylabel('g(w)', fontsize=12)\n",
    "    accepted_samples, accepted_samples_objective = random_sampling_minimum(inputs, n_samples, objective_value, n + 1)\n",
    "    ax.plot(np.array(accepted_samples), np.array(accepted_samples_objective), \"gD\", markersize=4)\n",
    "    ax.set_title(f'Number of Samples = {n_samples}')\n",
    "    print(f\"Case {n+1} Minimum Value is {min(accepted_samples_objective)}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_vector(w):\n",
    "\treturn np.dot(w, w.T) + 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiDimensional_sampling(number_samples, n_dimension, case, size_domain):\n",
    "    X = np.linspace(r_min, r_max, size_domain)\n",
    "    accepted_samples = []\n",
    "    accepted_samples_objective = []\n",
    "    minimum = 1e30\n",
    "    k = 0\n",
    "    stuck_flag = 0\n",
    "    stuck_iteration = 1e6\n",
    "    while k < number_samples:\n",
    "        stuck_flag = stuck_flag + 1\n",
    "        if stuck_flag == stuck_iteration:\n",
    "            print(f\"Case {case} exited since algorithm was stuck after {stuck_iteration} iterations\")\n",
    "            return accepted_samples, accepted_samples_objective\n",
    "        sample = np.zeros(n_dimension)\n",
    "        for i in range(0,n_dimension):\n",
    "            sample[i] = np.random.choice(X)\n",
    "        if objective_vector(sample) < minimum:\n",
    "            stuck_flag = 0\n",
    "            accepted_samples.append(sample)\n",
    "            minimum = objective_vector(sample)\n",
    "            accepted_samples_objective.append(minimum)\n",
    "            if abs(minimum-objective_value) <= 1e-2:\n",
    "                print(f\"Case {case} convergence is reached with {k+1} samples\")\n",
    "                return accepted_samples, accepted_samples_objective\n",
    "            k = k + 1   \n",
    "    return accepted_samples, accepted_samples_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = 20\n",
    "n_dimension_lst = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "xy_min = [-1, 1]\n",
    "xy_max = [-1, 1]\n",
    "r_min = -1; r_max = 1\n",
    "size_domain = 100\n",
    "\n",
    "run_times = np.zeros(len(n_dimension_lst))\n",
    "\n",
    "for case, n_dimension in enumerate(n_dimension_lst):\n",
    "    tic = time.time()\n",
    "    accepted_samples, accepted_samples_objective = multiDimensional_sampling(number_samples, n_dimension, case+1, size_domain)\n",
    "    toc = time.time()\n",
    "    print(f'Case {case+1}: Dimension is {n_dimension} with a reached minimum equal to {min(accepted_samples_objective)}')\n",
    "    run_times[case] = 1000 * (toc - tic)\n",
    "    print(f\"Duration of Case {case+1}: {run_times[case] : .4f} ms \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(n_dimension_lst, run_times, marker='o')\n",
    "plt.xlabel('dimension')\n",
    "plt.ylabel('run time (ms)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The defining characteristic of the *random local search* (or just *random search*) - as is the case with every local optimization method - is how the descent direction $\\mathbf{d}^{k-1}$ is chosen.\n",
    " \n",
    " \n",
    "- With random search we do (perhaps) the laziest possible thing: we look locally around the current point in a fixed number of random directions for a point that has a lower objective function value, and if we find one we move to it.  \n",
    "\n",
    "- More precisely, at the $k^{th}$ step of random search we pick a number $P$ of random directions to try out.  \n",
    "\n",
    "\n",
    "- Generating the $p^{th}$ random direction $\\mathbf{d}^p$ stemming from the previous step $\\mathbf{w}^{k-1}$ we have a candidate point to evaluate\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{\\text{candidate}} = \\mathbf{w}^{k-1} + \\mathbf{d}^{p}\n",
    "\\end{equation}\n",
    "\n",
    "- After evaluating all $P$ candidate points we pick the one that gives us the *smallest* objective function value i.e., the one with the index given by the smallest objective function value \n",
    "\n",
    "\\begin{equation}\n",
    "s = \\underset{p=1...P}{\\text{argmin}}\\,\\,g\\left(\\mathbf{w}^{k-1} +  \\mathbf{d}^p\\right)\n",
    "\\end{equation}\n",
    "\n",
    "- Finally, if best point found has a smaller objective function value than the current point i.e., if  $g\\left(\\mathbf{w}^{k-1} + \\mathbf{d}^s\\right) < g\\left(\\mathbf{w}^{k-1}\\right)$ then we move to the new point $\\mathbf{w}^k = \\mathbf{w}^{k-1} + \\mathbf{d}^s$, otherwise we examine another batch of $P$ random directions and try again.\n",
    "\n",
    "- Since we have no apriori reason for doing this at each step, to keep our random candidate directions consistent we can normalize them to have the same length e.g., length one. \n",
    "\n",
    "\n",
    "- If we use directions of unit-length in our algorithm - i.e., where $\\Vert \\mathbf{d} \\Vert_2 = 1$  always - this means that at each step of the algorithm we move a distance of length one since\n",
    "\n",
    "\\begin{equation}\n",
    "\\Vert \\mathbf{w}^k - \\mathbf{w}^{k-1} \\Vert_2 = \\Vert \\left(\\mathbf{w}^{k-1} + \\mathbf{d}\\right) - \\mathbf{w}^{k-1} \\Vert_2  = \\Vert \\mathbf{d}  \\Vert_2 = 1.\n",
    "\\end{equation}\n",
    "\n",
    "- From here we can adjust each step to have whatever length we desire by introducing a *steplength parameter* $\\alpha$ into each step to completely control how far we travel with each step.  This more general step looks like the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^k = \\mathbf{w}^{k-1} + \\alpha\\mathbf{d}^{\\,}\n",
    "\\end{equation}\n",
    "\n",
    "- The length of this step - using a unit-length directions - is now exactly equal to the steplength $\\alpha$, as\n",
    "\n",
    "\\begin{equation}\n",
    "\\Vert \\mathbf{w}^k - \\mathbf{w}^{k-1} \\Vert_2 = \\Vert \\left(\\mathbf{w}^{k-1} + \\alpha\\mathbf{d} \\right) - \\mathbf{w}^{k-1} \\Vert_2  = \\Vert \\alpha \\mathbf{d}  \\Vert_2 = \\alpha \\Vert \\mathbf{d}  \\Vert_2 = \\alpha\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- Now at the $k^{th}$ step we try out $P$ unit-length random directions - but scaled by the steplength parameter so that the distance we travel is actually $\\alpha$ - taking the one that provides the greatest decrease in function value.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms steps:\n",
    "**input**: Initial point $\\mathbf{w}^0$, maximum number of steps $K$, number of random samples per step $P$, a steplength $\\alpha$ or diminishing steplength rule\n",
    "\n",
    "**for** $k = 1 \\cdots K$:\n",
    "  * Compute $P$ unit length random directions $\\{\\mathbf{d}^p\\}_{p=1}^P$\n",
    "  * find $s = \\underset{p}{\\operatorname{\\argmin}} \\quad g(\\mathbf{w}^{k-1}+\\alpha\\mathbf{d}^p)$\n",
    "  * Set $\\mathbf{d}^k=\\mathbf{d}^s$\n",
    "  * Form a new point $\\mathbf{w}^k = \\mathbf{w}^{k-1} + \\alpha\\mathbf{d}^k$\n",
    "  * if $g(\\mathbf{w}^k) <g(\\mathbf{w}^{k-1})$\n",
    "    * $\\mathbf{w}^{k-1} \\leftarrow \\mathbf{w}^k$\n",
    "\n",
    "**output**: History of weights $\\{\\mathbf{w}^k\\}_{k=0}^K$ and corresponding function evaluations $\\{g(\\mathbf{w}^k)\\}_{k=0}^K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(g, alpha_choice, max_its, w, num_samples):\n",
    "    # run random search\n",
    "    weight_history = []         # container for weight history\n",
    "    cost_history = []           # container for corresponding cost function history\n",
    "    alpha = 0\n",
    "    for k in range(1, max_its + 1):        \n",
    "        # check if diminishing steplength rule used\n",
    "        if alpha_choice == 'diminishing':\n",
    "            alpha = 1 / k\n",
    "        else:\n",
    "            alpha = alpha_choice\n",
    "            \n",
    "        # record weights and cost evaluation\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w))\n",
    "        \n",
    "        # construct set of random unit directions\n",
    "        directions = np.random.randn(num_samples, np.size(w))\n",
    "        # norms = np.sqrt(np.sum(directions * directions, axis = 1))[:, np.newaxis]\n",
    "        norms = np.linalg.norm(directions, axis = 1)[:, np.newaxis]\n",
    "        directions = directions / norms   \n",
    "        \n",
    "        ### pick best descent direction\n",
    "        # compute all new candidate points\n",
    "        w_candidates = w + alpha * directions\n",
    "        \n",
    "        # evaluate all candidates\n",
    "        evals = np.array([g(w_val) for w_val in w_candidates])\n",
    "\n",
    "        # if we find a real descent direction take the step in its direction\n",
    "        ind = np.argmin(evals)\n",
    "        if g(w_candidates[ind]) < g(w):\n",
    "            # pluck out best descent direction\n",
    "            d = directions[ind,:]\n",
    "        \n",
    "            # take step\n",
    "            w = w + alpha * d\n",
    "        \n",
    "    # record weights and cost evaluation\n",
    "    weight_history.append(w)\n",
    "    cost_history.append(g(w))\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(g, weight_history, view, flag3D, title):\n",
    "    weights_steps_x = np.array([i[0] for i in weight_history])\n",
    "    weights_steps_y = np.array([i[1] for i in weight_history])\n",
    "    x = y = np.arange(-4.5, 4.5, 0.05)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    zs = np.array([g(np.array([x,y])) for x,y in zip(np.ravel(X), np.ravel(Y))])\n",
    "    Z = zs.reshape(X.shape)\n",
    "    if (flag3D):\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        ax = fig.add_subplot(111,projection='3d')\n",
    "        ax.quiver(weights_steps_x[:-1], weights_steps_y[:-1], np.zeros(weights_steps_x[:-1].shape[0]), weights_steps_x[1:]-weights_steps_x[:-1], weights_steps_y[1:]-weights_steps_y[:-1], np.zeros(weights_steps_x[:-1].shape[0]),\\\n",
    "            color='k')\n",
    "        ax.grid(False)\n",
    "        ax.plot_surface(X, Y, Z, alpha=0.8, cmap=cm.coolwarm, linewidth=10, antialiased=False)\n",
    "        ax.view_init(view[0], view[1])\n",
    "        ax.set_xlabel(r'$w_1$')\n",
    "        ax.set_ylabel(r'$w_2$')\n",
    "        ax.set_zlabel(r'$g(w)$')\n",
    "        ax.set_title(title,fontsize=16)\n",
    "    #ax.plot([0, 0], [-4.5,4.5], [0,0])\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    cp = plt.contourf(X, Y, Z, cmap='coolwarm')\n",
    "    plt.colorbar(cp)\n",
    "    plt.xlabel(r'$w_1$', fontsize=12)\n",
    "    plt.ylabel(r'$w_2$', fontsize=12)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.plot(weights_steps_x, weights_steps_y, 'sk', markersize=4)\n",
    "    plt.quiver(weights_steps_x[:-1], weights_steps_y[:-1], weights_steps_x[1:]-weights_steps_x[:-1], weights_steps_y[1:]-weights_steps_y[:-1], scale_units='xy', angles='xy', scale=1)\n",
    "    plt.plot(weights_steps_x[-1], weights_steps_y[-1], 'sy')\n",
    "    plt.title(title,fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search applied to minimize a simple quadratic\n",
    "The function being minimized is the simple quadratic $g(\\mathbf{w}) = \\mathbf{w}^T \\mathbf{w}^{\\,} + 2$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda w: np.dot(w.T, w) + 2\n",
    "alpha_choice = 1; w = np.array([3,4]); num_samples = 1000; max_its = 5; #for k in range(1,max_its+1): \n",
    "weight_history, cost_history = random_search(g, alpha_choice, max_its ,w, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(g, weight_history, view=[20,100], flag3D=True, title = 'Random Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(range(0,max_its+1), cost_history, 'dk-')\n",
    "plt.title(r'$g(w^k)$ vs. step $k$', fontsize=16)\n",
    "plt.xlabel('step k', fontsize=12)\n",
    "plt.ylabel(r'$g(w^k)$', fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing a function with many local minima using random search\n",
    "\n",
    "- In this example we show what one may need to do in order to find the global minimum of a function using (normalized) random local search. \n",
    "\n",
    "\n",
    "- For visualization purposes we use the single-input function $g(w) = \\text{sin}(3w) + 0.1w^2$.\n",
    "\n",
    "\n",
    "- We initialize two runs at $w_0^{(1)} = 4.5$ and $w_0^{(2)} = -1.5$.  For both runs we use a steplength of $\\alpha = 0.1$ fixed for all 10 iterations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda w: np.sin(3 * w) + 0.1 * w ** 2\n",
    "\n",
    "### do two random searchs with different starting points\n",
    "w1_initial = 4.5\n",
    "w2_initial = -1.5\n",
    "\n",
    "alpha_choice = 0.1; w = np.asarray([w1_initial]); num_samples = 10; max_its = 10;\n",
    "weight_history_1, cost_history_1 = random_search(g,alpha_choice,max_its,w,num_samples)\n",
    "weights_steps_x_1 = np.asarray([i[0] for i in weight_history_1])\n",
    "\n",
    "alpha_choice = 0.1; w = np.asarray([w2_initial]); num_samples = 10; max_its = 10;\n",
    "weight_history_2,cost_history_2 = random_search(g,alpha_choice,max_its,w,num_samples)\n",
    "weights_steps_x_2 = np.asarray([i[0] for i in weight_history_2])\n",
    "\n",
    "r_min = -5; r_max = 5\n",
    "plt.figure(figsize=(10,5))\n",
    "x = np.arange(r_min,r_max,0.01)\n",
    "plt.plot(x, g(x), 'k-')\n",
    "plt.xlabel('step k', fontsize=12)\n",
    "plt.ylabel(r'$g(w^k)$', fontsize=12)\n",
    "plt.xlim(-5,5)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x, g(x), 'k-')\n",
    "plt.plot(weight_history_1, cost_history_1, 'gx')\n",
    "plt.quiver(weights_steps_x_1[:-1], np.zeros(weights_steps_x_1[:-1].shape[0]), weights_steps_x_1[1:]-weights_steps_x_1[:-1], \\\n",
    "    np.zeros(weights_steps_x_1[1:].shape[0]), scale_units='xy', angles='xy', scale=1, color='g')\n",
    "plt.plot(weight_history_2, cost_history_2, 'rd')\n",
    "plt.quiver(weights_steps_x_2[:-1], np.zeros(weights_steps_x_2[:-1].shape[0]), weights_steps_x_2[1:]-weights_steps_x_2[:-1], \\\n",
    "    np.zeros(weights_steps_x_2[1:].shape[0]), scale_units='xy', angles='xy', scale=1, color='r')\n",
    "plt.plot(weights_steps_x_1[-1], g(weights_steps_x_1[-1]), 'xy', markersize=7)\n",
    "plt.plot(weights_steps_x_2[-1], g(weights_steps_x_2[-1]), 'dy', markersize=7)\n",
    "plt.plot(weights_steps_x_1[-1], 0, 'xy', markersize=7)\n",
    "plt.plot(weights_steps_x_2[-1], 0, 'dy', markersize=7)\n",
    "plt.xlabel('step k', fontsize=12)\n",
    "plt.ylabel(r'$g(w^k)$', fontsize=12)\n",
    "plt.xlim(-5,5)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit length steps fail to converge to global minimum\n",
    "\n",
    "- Here we re-run the random local search algorithm using the same simple quadratic and algorithm settings:\n",
    "\n",
    "- $g(w) = \\mathbf{w^T}\\mathbf{w} + 2$.\n",
    "\n",
    "- Now we initialize at the point $\\mathbf{w}^0 = \\begin{bmatrix} 1.5 \\\\ 2 \\end{bmatrix}$ which prevents the algorithm from reaching the function's global minimum.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda w: np.dot(w.T,w) + 2\n",
    "\n",
    "# run random search algorithm \n",
    "alpha_choice = 1; w = np.asarray([1.5, 2]); num_samples = 1000; max_its = 30;\n",
    "weight_history_1,cost_history_1 = random_search(g, alpha_choice, max_its, w, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(g, weight_history_1, view=[20,100],flag3D=True, title = 'Random Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(range(0,max_its+1), cost_history_1, 'dk-')\n",
    "plt.title(r'$g(w^k)$ vs. step $k$', fontsize=16)\n",
    "plt.xlabel('step k', fontsize=12)\n",
    "plt.ylabel(r'$g(w^k)$', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting the steplength parameter $\\alpha$ smaller we can look again make another run mirroring the one performed above, with much better results.  \n",
    "\n",
    "- $g(w) = \\mathbf{w^T}\\mathbf{w} + 2$.\n",
    "\n",
    "- Below we make the same run as above except now we set $\\alpha = 0.1$ for all steps.  Running the algorithm now we can see that it converges to a point much closer to the global minimum of the function at $\\mathbf{w} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda w: np.dot(w.T,w) + 2\n",
    "\n",
    "alpha_choice = 0.1; w = np.asarray([1.5,2]); num_samples = 1000; max_its = 30;\n",
    "weight_history_2,cost_history_2 = random_search(g, alpha_choice, max_its ,w, num_samples)\n",
    "\n",
    "plot_contours(g, weight_history_2, view=[20,100], flag3D=True, title = 'Random Search')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember however that we need to be careful in choosing the steplength value with this simple quadratic, and by extension any general function.  \n",
    "\n",
    "- $g(w) = \\mathbf{w^T}\\mathbf{w} + 2$.\n",
    "\n",
    "- If - for example - we run the same experiment again but cut the steplength down to $\\alpha = 0.01$ we do not reach a point anywher near the global minimum, as we show by performing the same run but setting $\\alpha$ to this value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda w: np.dot(w.T,w) + 2\n",
    "\n",
    "# run random search algorithm \n",
    "alpha_choice = 0.01; w = np.asarray([1.5,2]); num_samples = 1000; max_its = 30;\n",
    "weight_history_3,cost_history_3 = random_search(g, alpha_choice, max_its, w, num_samples)\n",
    "\n",
    "plot_contours(g, weight_history_3, view=[20,100], flag3D=True, title = 'Random Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(range(0,max_its+1), cost_history_2, 'dk-', label = r'$\\alpha=0.1$')\n",
    "plt.plot(range(0,max_its+1), cost_history_3, 'sr-',  label = r'$\\alpha=0.01$')\n",
    "plt.title(r'$g(w^k)$ vs. step $k$', fontsize=16)\n",
    "plt.xlabel('step k', fontsize=12)\n",
    "plt.ylabel(r'$g(w^k)$', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Search and Descent\n",
    "\n",
    "- The *coordinate search* algorithm takes the theme of descent direction search and - instead of searching randomly - restricts the set of directions to the coordinate axes of the input space alone.  \n",
    "\n",
    "\n",
    "- While this significantly limits the kinds of descent directions we can recover it far more scalable than seeking out a good descent direction at random, and opens the search-approach to determining descent directions to usage with higher dimensional input functions.\n",
    "\n",
    "- This means - in particular - each pair of candidate points using a single standard basis direction looks like\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{\\text{candidate}} = \\mathbf{w}^{k-1} \\pm \\alpha \\mathbf{e}_n.\n",
    "\\end{equation}\n",
    "\n",
    "- It is this restricted set of directions we are searching over that distinguishes the coordinate search approach from the random search approach described previously, where the set of directions at each step was made up of random directions. \n",
    "\n",
    "\n",
    "- While the diversity of the coordinate axes may limit the effectiveness of the possible descent directions it can encounter and thus require more steps to determine an approximate minimum, the restricted search makes coordinate search far more scalable than the random search method since at each step only $2N$ directions must be tested."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Search Algorithm\n",
    "- **Input**: Initial point $\\mathbf{w}^0$, maximum number of steps $K$, a steplength $\\alpha$ or diminishing steplength rule, take the set of directions $\\mathbf{d}^{2n-1}$ and $\\mathbf{d}^{2n} = -\\mathbf{e}_n$ for $n=1 \\cdots N$ and set $P=2N$\n",
    "\n",
    "$\\quad$ **for** $k = 1 \\cdots K$:\n",
    "  * find $s = \\underset{p}{\\operatorname{\\argmin}} g(\\mathbf{w}^{k-1}+\\alpha\\mathbf{d}^p)$\n",
    "  * Set $\\mathbf{d}^k=\\mathbf{d}^s$\n",
    "  * Form a new point $\\mathbf{w}^k = \\mathbf{w}^{k-1} + \\alpha\\mathbf{d}^k$\n",
    "  * if $g(\\mathbf{w}^k)<g(\\mathbf{w}^{k-1})$\n",
    "    * $\\mathbf{w}^{k-1} \\leftarrow \\mathbf{w}^k$\n",
    "\n",
    "**output**: History of weights $\\{\\mathbf{w}^k\\}_{k=0}^K$ and corresponding function evaluations $\\{g(\\mathbf{w}^k)\\}_{k=0}^K$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Descent Algorithm\n",
    "\n",
    "A slight twist on the coordinate search produces a much more effective algorithm at precisely the same computational cost. Instead of collecting each coordinate direction (along with its negative), and then choosing a single best direction from this entire set, we can simply examine one coordinate direction (and its negative) at a time and step in this direction if it produces descent. Whereas with coordinate search we evaluate the cost function $2N$ times (once per coordinate direction and its negative) to produce a single step, this alternative takes the same number of function evaluations but potentially moves $N$ steps in doing so. In other words this means that for precisely the same cost as coordinate search we can (potentially) descent much faster with coordinate descent.\n",
    "\n",
    "This twist on the coordinate search approach is called a coordinate descent, since each step evaluates a single coordinate direction and decides whether or not to move in this direction alone. This particular algorithm - while itself being the most effective zero order method we have seen thus far by far\n",
    "\n",
    "**Input**: Initial point $\\mathbf{w}^0$, maximum number of steps $K$, a steplength $\\alpha$ or diminishing steplength rule\n",
    "\n",
    "**for** $k = 1 \\cdots K$:\n",
    "\n",
    "$\\quad$ **for** $n = 1 \\cdots N$:\n",
    "  * find $s = \\underset{}{\\operatorname{\\argmin}} \\{g(\\mathbf{w}^{k+n-1}+\\alpha\\mathbf{e}_n), g(\\mathbf{w}^{k+n-1}-\\alpha\\mathbf{e}_n)\\}$\n",
    "  * Set $\\mathbf{d}^{k+n}=(-1)^s\\mathbf{e}_n$\n",
    "  * Form a new point $\\mathbf{w}^{k+n} = \\mathbf{w}^{k+n-1} + \\alpha\\mathbf{d}^{k+n}$\n",
    "  * if $g(\\mathbf{w}^{k+n})<g(\\mathbf{w}^{k+n-1})$\n",
    "    * $\\mathbf{w}^{k+n-1} \\leftarrow \\mathbf{w}^{k+n}$\n",
    "\n",
    "**output**: History of weights $\\{\\mathbf{w}^k\\}_{k=0}^{KN}$ and corresponding function evaluations $\\{g(\\mathbf{w}^k)\\}_{k=0}^{KN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero order coordinate search\n",
    "def coordinate_search(g,alpha_choice,max_its,w):\n",
    "    # construct set of all coordinate directions\n",
    "    #directions_plus = np.eye(np.size(w),np.size(w))\n",
    "    directions_plus = np.eye(np.size(w))\n",
    "    #directions_minus = - np.eye(np.size(w),np.size(w))\n",
    "    directions_minus = - np.eye(np.size(w))\n",
    "    directions = np.concatenate((directions_plus,directions_minus),axis=0)\n",
    "        \n",
    "    # run coordinate search\n",
    "    weight_history = []         # container for weight history\n",
    "    cost_history = []           # container for corresponding cost function history\n",
    "    alpha = 0\n",
    "    for k in range(1,max_its+1):        \n",
    "        # check if diminishing steplength rule used\n",
    "        if alpha_choice == 'diminishing':\n",
    "            alpha = 1/float(k)\n",
    "        else:\n",
    "            alpha = alpha_choice\n",
    "            \n",
    "        # record weights and cost evaluation\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w))\n",
    "        \n",
    "        ### pick best descent direction\n",
    "        # compute all new candidate points\n",
    "        w_candidates = w + alpha*directions\n",
    "        \n",
    "        # evaluate all candidates\n",
    "        evals = np.array([g(w_val) for w_val in w_candidates])\n",
    "\n",
    "        # if we find a real descent direction take the step in its direction\n",
    "        ind = np.argmin(evals)\n",
    "        if g(w_candidates[ind]) < g(w):\n",
    "            # pluck out best descent direction\n",
    "            d = directions[ind,:]\n",
    "        \n",
    "            # take step\n",
    "            w = w + alpha*d\n",
    "        \n",
    "    # record weights and cost evaluation\n",
    "    weight_history.append(w)\n",
    "    cost_history.append(g(w))\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_descent_zero_order(g,alpha_choice,max_its,w):  \n",
    "    # run coordinate search\n",
    "    N = np.size(w)\n",
    "    weight_history = []         # container for weight history\n",
    "    cost_history = []           # container for corresponding cost function history\n",
    "    alpha = 0\n",
    "    for k in range(1,max_its+1):        \n",
    "        # check if diminishing steplength rule used\n",
    "        if alpha_choice == 'diminishing':\n",
    "            alpha = 1/float(k)\n",
    "        else:\n",
    "            alpha = alpha_choice\n",
    "        \n",
    "        # random shuffle of coordinates\n",
    "        c = np.random.permutation(N)\n",
    "        \n",
    "        # forming the direction matrix out of the loop\n",
    "        cost = g(w)\n",
    "        \n",
    "        # loop over each coordinate direction\n",
    "        for n in range(N):\n",
    "            direction = np.zeros((N,1)).flatten()\n",
    "            direction[c[n]] = 1\n",
    "    \n",
    "            # record weights and cost evaluation\n",
    "            weight_history.append(w)\n",
    "            cost_history.append(cost)\n",
    "\n",
    "            # evaluate all candidates\n",
    "            evals =  [g(w + alpha*direction)]\n",
    "            evals.append(g(w - alpha*direction))\n",
    "            evals = np.array(evals)\n",
    "\n",
    "            # if we find a real descent direction take the step in its direction\n",
    "            ind = np.argmin(evals)\n",
    "            if evals[ind] < cost_history[-1]:\n",
    "                # take step\n",
    "                w = w + ((-1)**(ind))*alpha*direction\n",
    "                cost = evals[ind]\n",
    "        \n",
    "    # record weights and cost evaluation\n",
    "    weight_history.append(w)\n",
    "    cost_history.append(g(w))\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Search vs. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however that while it may take the coordinate search method more steps to reach the minimum of this particular function, it is still far more computationally efficient than the random search run shown above. This is because when it comes down to the cost per step of each algorithm\n",
    "\n",
    "- with coordinate search: checking the positive and negative directions along the coordinate axes in  N\n",
    " dimensional space costs essentially  2N\n",
    "  function evaluations (one per direction)\n",
    "- with random search: checking  P\n",
    "  random directions essentially costs  P\n",
    "  function evaluations per step (again, one per direction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(w) = \\mathbf{w^T}\\mathbf{w} + 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "g = lambda w: np.dot(w.T,w) + 2\n",
    "\n",
    "tic = time.time()  # capture start time\n",
    "# run random search algorithm \n",
    "alpha_choice = 1; w = np.array([3,4]); num_samples = 1000; max_its = 5;\n",
    "weight_history_1,cost_history_1 = random_search(g,alpha_choice,max_its,w,num_samples)\n",
    "toc = time.time()  # capture end time\n",
    "print(f\"Duration of Random Search: {1000*(toc-tic):.4f} ms \")\n",
    "print(f\"Final Cost of Random Search: {cost_history_1[-1]}\")\n",
    "\n",
    "tic = time.time()  # capture start time\n",
    "# run coordinate search algorithm \n",
    "alpha_choice = 1; w = np.array([3,4]); max_its = 7;\n",
    "weight_history_2,cost_history_2 = coordinate_search(g,alpha_choice,max_its,w)\n",
    "toc = time.time()  # capture end time\n",
    "print(f\"Duration of Coordinate Search: {1000*(toc-tic):.4f} ms \")\n",
    "print(f\"Final Cost of Coordinate Search: {cost_history_2[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinate search is a local optimization method, and so we still must properly set the steplength parameterÂ¶\n",
    "\n",
    "$g(w_1,w_2) = 0.26(w_1^2+w_2^2) -0.48w_1w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda w: 0.26*(w[0]**2 + w[1]**2) - 0.48*w[0]*w[1]\n",
    "alpha_choice = 1; w = np.array([3,4]); num_samples = 1000; max_its = 5;\n",
    "weight_history_1,cost_history_1 = random_search(g,alpha_choice,max_its,w,num_samples)\n",
    "plot_contours(g, weight_history_1, view=[20,50], flag3D = False, title = 'Random Search')\n",
    "alpha_choice = 1; w = np.array([3,4]); max_its = 5;\n",
    "weight_history_2,cost_history_2 = coordinate_search(g,alpha_choice,max_its,w)\n",
    "plot_contours(g, weight_history_2, view=[20,50], flag3D = False, title = 'Coordinate Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Search vs. Coordinate Descent\n",
    "\n",
    "$g(w_1,w_2) = 0.26(w_1^2+w_2^2) -0.48w_1w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda w: 0.26*(w[0]**2 + w[1]**2) - 0.48*w[0]*w[1]\n",
    "\n",
    "# run coordinate search algorithm \n",
    "alpha_choice = 'diminishing'; w = np.array([3,4]); max_its = 40;\n",
    "weight_history_1,cost_history_1 = coordinate_search(g,alpha_choice,max_its,w)\n",
    "plot_contours(g, weight_history_1, view=[20,50], flag3D = False, title = 'Coordinate Search')\n",
    "# run coordinate descent algorithm\n",
    "alpha_choice = 'diminishing'; w = np.array([3,4]); max_its = 40;\n",
    "weight_history_2,cost_history_2 = coordinate_descent_zero_order(g,alpha_choice,max_its,w)\n",
    "#plot_contours(g, weight_history_2, view=[20,50], flag3D = False, title = 'Coordinate Descent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(range(0,max_its+1), cost_history_1, label = 'Coordinate Search')\n",
    "plt.plot(range(0, len(cost_history_2)), cost_history_2,  label = 'Coordinate Descent')\n",
    "plt.title(r'$g(w^k)$ vs. step $k$', fontsize=16)\n",
    "plt.xlabel('step k', fontsize=12)\n",
    "plt.ylabel(r'$g(w^k)$', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(w_1,w_2) = 0.26(w_1^2+w_2^2) -0.48w_1w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda w: 0.26*(w[0]**2 + w[1]**2) - 0.48*w[0]*w[1]\n",
    "\n",
    "# run coordinate search algorithm \n",
    "alpha_choice = 'diminishing'; w = np.array([3,4]); max_its = 100;\n",
    "weight_history,cost_history = coordinate_descent_zero_order(g,alpha_choice,max_its,w)\n",
    "\n",
    "plot_contours(g, weight_history, view=[20,50], flag3D = False, title = 'Coordinate Descent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Order Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent and The First order Optimality Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_descent_for_quadratic(g,w,max_its,a,b,C):\n",
    "    '''\n",
    "    Coordinate descent wrapper for general quadratic function. Here\n",
    "    \n",
    "    a - a constant\n",
    "    b - an Nx1 vector\n",
    "    C - an NxN matrix (symmetric and all nonnegative eigenvalues)\n",
    "    '''\n",
    "        \n",
    "    # record weights and cost history \n",
    "    weight_history = [copy.deepcopy(w)]     \n",
    "    cost_history = [g(w)]\n",
    "    N = np.size(w)\n",
    "    \n",
    "    # outer loop - each is a sweep through every variable once\n",
    "    for k in range(max_its):\n",
    "        # inner loop - each is a single variable update\n",
    "        for n in range(N):\n",
    "            w[n] = -(np.dot(C[n,:],w) - C[n,n]*w[n] + 0.5*b[n])/float(C[n,n])\n",
    "            \n",
    "            # record weights and cost value at each step\n",
    "            weight_history.append(copy.deepcopy(w))\n",
    "            cost_history.append(g(w))\n",
    "\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we use this algorithm to minimize the simple quadratic \n",
    "\n",
    "\\begin{equation}\n",
    "g(w_0,w_1) = w_0^2 + w_1^2 + 2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- We initialize at $\\mathbf{w} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$ and run $1$ iteration of the algorithm - that is all it takes to perfectly minimize the function, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "# define constants for a N=2 input quadratic\n",
    "a = 2\n",
    "b = np.zeros((2,1))\n",
    "C = np.eye(2)\n",
    "\n",
    "# a quadratic function defined using the constants above\n",
    "g = lambda w: (a + np.dot(b.T,w) + np.dot(np.dot(w.T,C),w))[0]\n",
    "\n",
    "# initialization\n",
    "w = np.array([3,4])\n",
    "\n",
    "# run your alternating descent code\n",
    "max_its = 1\n",
    "weight_history,cost_history = coordinate_descent_for_quadratic(g,w,max_its,a,b,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(g, weight_history, view=[20,50], flag3D = False, title = 'Coordinate Descent on a convex quadratic function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below we show a run of $2$ iterations of the method at the same initial point for the quadratic\n",
    "\n",
    "\\begin{equation}\n",
    "g(w_0,w_1) = 2w_0^2 + 2w_1^2 + 2w_0w_1 + 20\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "# define constants for a N=2 input quadratic\n",
    "a = 20\n",
    "b = np.zeros((2,1))\n",
    "C = np.array([[2,1],[1,2]])\n",
    "\n",
    "# a quadratic function defined using the constants above\n",
    "g = lambda w: (a + np.dot(b.T,w) + np.dot(np.dot(w.T,C),w))[0]\n",
    "\n",
    "# initialization\n",
    "w = np.array([3,4])\n",
    "\n",
    "# run your alternating descent code\n",
    "max_its = 2\n",
    "weight_history,cost_history = coordinate_descent_for_quadratic(g,w,max_its,a,b,C)\n",
    "\n",
    "plot_contours(g, weight_history, view=[20,50], flag3D = False, title = 'Coordinate Descent on a convex quadratic function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient descent algorithm\n",
    "\n",
    "1. function $g$, steplength $\\alpha$, maximum number of steps $K$, and initial point $\\mathbf{w}^0$ <br/>\n",
    "\n",
    "2.  <code>for</code> $\\,\\,k = 1...K$<br/>\n",
    "\n",
    "    * $\\mathbf{w}^k = \\mathbf{w}^{k-1} - \\alpha \\nabla g\\left(\\mathbf{w}^{k-1}\\right)$ <br/>\n",
    "\n",
    "4.  <strong>output:</strong> history of weights $\\left\\{\\mathbf{w}^{k}\\right\\}_{k=0}^K$ and corresponding function evaluations $\\left\\{g\\left(\\mathbf{w}^{k}\\right)\\right\\}_{k=0}^K$ <br/>\n",
    "\n",
    "<hr style=\"height:1px;border:none;color:#555;background-color:#555;\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# using an automatic differentiator - like the one imported via the statement below - makes coding up gradient descent a breeze\n",
    "from autograd import numpy as np\n",
    "from autograd import value_and_grad \n",
    "\n",
    "# gradient descent function - inputs: g (input function), alpha (steplength parameter), max_its (maximum number of iterations), w (initialization)\n",
    "def gradient_descent(g,alpha_choice,max_its,w):\n",
    "    # compute the gradient function of our input function - note this is a function too\n",
    "    # that - when evaluated - returns both the gradient and function evaluations (remember\n",
    "    # as discussed in Chapter 3 we always ge the function evaluation 'for free' when we use\n",
    "    # an Automatic Differntiator to evaluate the gradient)\n",
    "    gradient = value_and_grad(g)\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    weight_history = []      # container for weight history\n",
    "    cost_history = []        # container for corresponding cost function history\n",
    "    alpha = 0\n",
    "    for k in range(1,max_its+1):\n",
    "        # check if diminishing steplength rule used\n",
    "        if alpha_choice == 'diminishing':\n",
    "            alpha = 1/float(k)\n",
    "        else:\n",
    "            alpha = alpha_choice\n",
    "        \n",
    "        # evaluate the gradient, store current weights and cost function value\n",
    "        cost_eval,grad_eval = gradient(w)\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(cost_eval)\n",
    "\n",
    "        # take gradient descent step\n",
    "        w = w - alpha*grad_eval\n",
    "            \n",
    "    # collect final weights\n",
    "    weight_history.append(w)\n",
    "    # compute final cost function value via g itself (since we aren't computing \n",
    "    # the gradient at the final step we don't get the final cost function value \n",
    "    # via the Automatic Differentiatoor) \n",
    "    cost_history.append(g(w))  \n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we show the result of running gradient descent several times to minimize the function\n",
    "\n",
    "\\begin{equation}\n",
    "g(w) = \\text{sin}(3w) + 0.1w^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- For general non-convex functions like this one, several runs (of any local optimization method) can be necessary to determine points near global minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(w):\n",
    "    return np.sin(3*w) + 0.1*w**2\n",
    "\n",
    "# run gradient descent\n",
    "alpha = 0.05; w = 4.5; max_its = 10;\n",
    "weight_history_1,cost_history_1 = gradient_descent(g,alpha,max_its,w)\n",
    "weights_steps_x_1 = np.array(weight_history_1)\n",
    "\n",
    "alpha = 0.05; w = -1.5; max_its = 10;\n",
    "weight_history_2,cost_history_2 = gradient_descent(g,alpha,max_its,w)\n",
    "weights_steps_x_2 = np.array(weight_history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_min = -5; r_max = 5\n",
    "plt.figure(figsize=(10,5))\n",
    "x = np.arange(r_min,r_max,0.01)\n",
    "plt.plot(x, g(x), 'k-')\n",
    "plt.xlabel('step k', fontsize=12)\n",
    "plt.ylabel(r'$g(w^k)$', fontsize=12)\n",
    "plt.xlim(-5,5)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x, g(x), 'k-')\n",
    "plt.plot(weight_history_1, cost_history_1, 'gx')\n",
    "plt.quiver(weights_steps_x_1[:-1], np.zeros(weights_steps_x_1[:-1].shape[0]), weights_steps_x_1[1:]-weights_steps_x_1[:-1], \\\n",
    "    np.zeros(weights_steps_x_1[1:].shape[0]), scale_units='xy', angles='xy', scale=1, color='g')\n",
    "plt.plot(weight_history_2, cost_history_2, 'rd')\n",
    "plt.quiver(weights_steps_x_2[:-1], np.zeros(weights_steps_x_2[:-1].shape[0]), weights_steps_x_2[1:]-weights_steps_x_2[:-1], \\\n",
    "    np.zeros(weights_steps_x_2[1:].shape[0]), scale_units='xy', angles='xy', scale=1, color='r')\n",
    "plt.plot(weights_steps_x_1[-1], g(weights_steps_x_1[-1]), 'xy', markersize=7)\n",
    "plt.plot(weights_steps_x_2[-1], g(weights_steps_x_2[-1]), 'dy', markersize=7)\n",
    "plt.plot(weights_steps_x_1[-1], 0, 'xy', markersize=7)\n",
    "plt.plot(weights_steps_x_2[-1], 0, 'dy', markersize=7)\n",
    "plt.xlabel('step k', fontsize=12)\n",
    "plt.ylabel(r'$g(w^k)$', fontsize=12)\n",
    "plt.xlim(-5,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(range(0,max_its+1), cost_history_1, 'dg-', label = r'Run $1$')\n",
    "plt.plot(range(0,max_its+1), cost_history_2, 'sr-',  label = r'Run $2$')\n",
    "plt.title(r'$g(w^k)$ vs. step $k$', fontsize=16)\n",
    "plt.xlabel('step k', fontsize=12)\n",
    "plt.ylabel(r'$g(w^k)$', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using an automatic differentiator - like the one imported via the statement below - makes coding up gradient descent a breeze\n",
    "from autograd import grad \n",
    "from autograd import hessian\n",
    "\n",
    "# newtons method function - inputs: g (input function), max_its (maximum number of iterations), w (initialization)\n",
    "def newtons_method(g,max_its,w,**kwargs):\n",
    "    # compute gradient module using autograd\n",
    "    gradient = grad(g)\n",
    "    hess = hessian(g)\n",
    "    \n",
    "    # set numericxal stability parameter / regularization parameter\n",
    "    epsilon = 10**(-7)\n",
    "    if 'epsilon' in kwargs:\n",
    "        beta = kwargs['epsilon']\n",
    "\n",
    "    # run the newtons method loop\n",
    "    weight_history = [w]           # container for weight history\n",
    "    cost_history = [g(w)]          # container for corresponding cost function history\n",
    "    for k in range(max_its):\n",
    "        # evaluate the gradient and hessian\n",
    "        grad_eval = gradient(w)\n",
    "        hess_eval = hess(w)\n",
    "\n",
    "        # reshape hessian to square matrix for numpy linalg functionality\n",
    "        hess_eval.shape = (int((np.size(hess_eval))**(0.5)),int((np.size(hess_eval))**(0.5)))\n",
    "        \n",
    "        # solve second order system system for weight update\n",
    "        A = hess_eval + epsilon*np.eye(w.size)\n",
    "        b = grad_eval\n",
    "        w = np.linalg.solve(A,np.dot(A,w) - b)\n",
    "        \n",
    "        # record weight and cost\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w))\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda w: 0.26*(w[0]**2 + w[1]**2) - 0.48*w[0]*w[1]\n",
    "alpha = 1; w = np.array([4.0, 2.0]); max_its = 100;\n",
    "weight_history_1,cost_history_1 = gradient_descent(g,alpha,max_its,w)\n",
    "weights_steps_x_1 = np.array([i[0] for i in weight_history_1])\n",
    "\n",
    "#plot_contours(g, weight_history_1, view=[20,50], flag3D = False, title = 'Gradient Descent')\n",
    "\n",
    "w = np.array([4.0, 2.0]); max_its = 1;\n",
    "weight_history_2,cost_history_2 = newtons_method(g,max_its,w)\n",
    "weights_steps_x_2 = np.array([i[0] for i in weight_history_2])\n",
    "\n",
    "plot_contours(g, weight_history_2, view=[20,50], flag3D = False, title = 'Newton Method')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mls23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f2b55be8af22e299bb14a990e5306b14c0e04e0c371124ebf94aa533852bd32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
